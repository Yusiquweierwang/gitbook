[toc]

### 教材

雷明--机器学习原理--机器学习的数学
李航--统计学习方法
周志华 机器学习
机器学习公式详解

1-18 章，第 3 章不单独讲授。

![Alt text](https://pic4.zhimg.com/80/v2-9821a17870f505646c50bf39c6c7a1d7_720w.webp)

## 基础

### 回归和分类

![1699881126090](image/index/1699881126090.png)

**误差**
![1699881216888](image/index/1699881216888.png)

### 损失函数

**残差**
![1699881282060](image/index/1699881282060.png)

![1699881291806](image/index/1699881291806.png)
损失函数：残差平方和
![1699881335361](image/index/1699881335361.png)

β0 和 β1 怎么算出来？

残差平方和公式图像
![1699881463411](image/index/1699881463411.png)

![1699882375077](image/index/1699882375077.jpg)

![1699881493110](image/index/1699881493110.png)

![1699881579124](image/index/1699881579124.png)

## 用到数学

[label](https://mmbiz.qpic.cn/mmbiz_jpg/75DkJnThACnTA9mJlVxPZ0jXfeg8adewErpqRPczFKndWFboAzFBvEVCUJw0VV7giaFQfq4THb7ScWz90FYwgKA/640?wx_fmt%3Djpeg%26wxfrom%3D5%26wx_lazy%3D1%26wx_co%3D1)

![1695390500465](image/index/1695390500465.png)

### 微积分

导数和偏导数
梯度向量
极值定理
雅可比矩阵
hessian 矩阵
凸函数定义和判断方法
泰勒
拉格朗日求解带等式约束的极值问题

### 线性代数

向量 -- 加减 数乘 转置 内积
范数
矩阵
逆矩阵
行列式
二次型
正定性
特征值 特征向量
奇异值分解
共轭梯度法

### 概率论

随机事件
随机变量和概率分布
条件概率和贝叶斯
常用概率分布
随机变量均值和方差，协方差
随机变量独立性
最大似然估计

- 判定规则
- 监督分类/非监督分类
- 训练
- 训练样本

ImageNet

机器学习本质：模型的选择和模型参数的确立

数据：

- 音频：向量
- 数字图像
  - 二值图像：一个矩阵
  - 灰度图像：一个矩阵(0-255)
  - 彩色图像：三个矩阵
  - 遥感图像：多矩阵
- 视频：很多彩色图像
- 文本

补充
传统模式识别系统
![1695262373150](image/index/1695262373150.png)
特征是什么？
手工设计特征
自动学习特征

## 变换

affine 仿射变换

### 数据集划分

- 训练集
- 测试集
- 验证集

d 维空间中任意一点：

凹函数、凸函数：解决局部最优是否全局最优问题。

两个向量内积：两个向量之间夹角的余弦值。
几何意义——投影
逆矩阵

偏导数
梯度
梯度为 0 的点称多元函数的驻点。
梯度方向（反方向）是函数值上升（下降）最快的方向。

hessian 矩阵
多元凹/凸函数
正定矩阵

![Alt text](https://img-blog.csdnimg.cn/20200528231745969.png?x-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hvbGx5X1pfUF9G%2Csize_16%2Ccolor_FFFFFF%2Ct_70)

![Alt text](https://pic4.zhimg.com/80/v2-cf220fa82b917a311e9cab95e6758483_720w.webp)
正交矩阵——戈列向量都是单位向量，且两两正交

主成分分析/计算机视觉中奇异值分解--是对成矩阵对角化问题

二次型

验证公式

最优化方法
算法依据：寻找梯度值为 0 的点
解析解和数值解
使用迭代法寻找最优解

梯度下降法

数值优化算法面临问题
梯度为 0 的点可能：
是局部机制而非全局
甚至不是局部极值

## 贝叶斯

## 梯度下降法

![1696607846498](image/index/1696607846498.png)

![1696609137620](image/index/1696609137620.png)

![1696609160034](image/index/1696609160034.png)

![1696609192489](image/index/1696609192489.png)

## 最小二乘拟合

## 决策树

决策树是个从根到叶的递归过程，在每个中间节点寻找划分属性，重要的是**设置停止条件**：

### sklearn 中决策树

![1698803306731](image/index/1698803306731.png)
sklearn 基本建模原理：
![1698803531326](image/index/1698803531326.png)
![1698803556981](image/index/1698803556981.png)

`criterion`

- 输入 entropy ， 使用信息熵
- 输入 gini ， 用基尼系数

![1698804063148](image/index/1698804063148.png)

### 决策树基础

关键问题：

- ![1697076823580](image/index/1697076823580.png)

![Alt text](https://pic3.zhimg.com/v2-68dcbdc8949d606955f0bc2acd24614e_b.jpg)

### ID3-信息增益

熵 entropy-系统由不稳定态到稳定态所需要丢失的部分
![Alt text](https://pic2.zhimg.com/80/v2-738252aa284e221095bbc80fd17e98b1_720w.webp)
信息增益=划分之前的信息熵-划分之后的信息熵，越大意味着得到的信息越多，再往下的划分就少了。

### CART 决策树

### GINI 指数

gini：从数据集 D 中随机抽取两个样本，类别标记不一致的概率。GINI(D)越小，数据集 D 纯度越高。
![1697076980525](image/index/1697076980525.png)

走到栈底最大的值

### 决策树的剪枝

**过拟合 / 欠拟合**
过分贴合数据集

## 随机森林

- 集成学习算法，由多棵决策树组成
- 用对训练集随机抽样得到的样本训练决策树
- 不仅对训练集随机抽样，还对特性向量的分量随机抽

![1698854711812](image/index/1698854711812.png)
![1698854563322](image/index/1698854563322.png)

### 控制基评估器的参数

![1698854970045](image/index/1698854970045.png)

![1698856194455](image/index/1698856194455.png)

### 集成学习

**bootstrap 抽样**

- 如果训练集大小为 N，对于每棵树而言，随机且有放回地从训练集中的抽取 N 个训练样本（就是 bootstrap sample 方法, 拔靴法采样）作为该树的训练集；从这里我们可以知道 k：每棵树的训练集都是不同的，而且里面包含重复的训练样本

没有被抽到的数据：包外数据

**bagging(bootstrap aggregating)算法**

### 训练算法

- 样本随机抽样：均匀分布的随机数
- 特征分量随机抽样：随机洗牌
- 如何确定决策树数量？
  观察包外误差，在他稳定后停止训练新的决策树
- 如何确定特征数量？

## boosting 算法

从弱学习方法触发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。

### adaboost 算法

验证集作用

为社么验证臭皮匠数量要在验证集上观察，而不是测试集上？

## 预剪枝和后剪枝 吧吧

## K 近邻算法

基本思想：

- k 近邻
- 最近邻

懒惰学习

三要素：
距离度量
k 值选择
决策规则

![1698285738456](image/index/1698285738456.png)

交叉验证

kd 树

- 线性扫描的弊端（linear scan）
- 二叉搜索树

## 线性回归

### logistic 回归--多分类学习方法

![1698888831209](image/index/1698888831209.png)
![1698888840642](image/index/1698888840642.png)

![1698888849178](image/index/1698888849178.png)

![1698888866192](image/index/1698888866192.png)

![1698888874307](image/index/1698888874307.png)

p1 / p2 分别表示模型判断属于第一第二类别的概率。

训练目标：
确定 w 和 b，使[p1+p2+p3+p4+p5]最大，每个 p 越靠近 1.
/
找出 w 和 b，使目标函数 l(w) = ∑lnp 最小

### softmax 回归

先处理成 e^t 即处理成正数

参数有(D+1)K 个。

![1698889941143](image/index/1698889941143.png)

此上述两种模型都是广义线性模型，先做线性变化，在做非线性变换。
目的————把非凸函数转换为凸函数。

## 人工神经网络

![1699492745762](image/index/1699492745762.png)

![1699492762782](image/index/1699492762782.png)

![1699492658993](image/index/1699492658993.png)
转换为 e 保证转换为正数

激活函数是非线性是因为

![1699493756211](image/index/1699493756211.png)

![1699493796179](image/index/1699493796179.png).................................................
`针对此，有4*4+5*10个参数`

回归问题：预测模型
分类问题：预测输进来的是什么

![1699495699943](image/index/1699495699943.png)

![1699495969703](image/index/1699495969703.png)

![1699496522089](image/index/1699496522089.png)

![1699496533274](image/index/1699496533274.png)

![1699496568253](image/index/1699496568253.png)

![1699496579395](image/index/1699496579395.png)

![1699497158112](image/index/1699497158112.png)
对数字问题分类

## 支持向量机
